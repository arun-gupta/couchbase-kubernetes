= Couchbase Cluster with PetSet and PortWorx

== Why Portworx?

. *Container granular volumes* - Portworx can take multiple EBS volumes per host and aggregate the capacity and derive container granular virtual (soft) volumes per container.
. *Cross Availability Zone HA* - Portworx will protect the data, at block level, across multiple compute instances across availability zones.  As replication controllers restart pods on different nodes, the data will still be highly available on those nodes.
. *Support for enterprise data operations* - Portworx implements container granular snapshots, class of service, tiering on top of the available physical volumes.
. *Ease of deployment and provisioning* - Portworx itself is deployed as a container and integrated with the orchestration tools.  DevOps can programmatically provision container granular storage with any property such as size, class of service, encryption key etc.

== Portworx on Kubernetes

. Start Kubernetes cluster:
+
```
export KUBERNETES_PROVIDER=aws
KUBE_OS_DISTRIBUTION=wily NODE_SIZE=m3.medium MASTER_SIZE=m3.medium ./cluster/kube-up.sh
```
+
.. Default distribution is Debian and had issues starting up Portworx container
.. Default `NODE_SIZE` is `t2.medium`. This is not sufficient for running Couchbase node and PWX container.
. Create EBS volume using the command: `aws ec2 create-volume --region us-west-2 --availability-zone us-west-2a --volume-type io1 --iops 1000 --size 128`
.. Run the command 4 times. One volume is attached to each node.
. Get the volume ids: `vol-df734157`, `vol-ce734146`, `vol-ec734164`, `vol-cf734147`
. Get the instance ids for minions: `i-b27ee627`, `i-b37ee626`, `i-b47ee621`, `i-b57ee620`
. Make sure AWS CLI is configured correctly for `us-west-2` region: `aws configure`
. Attach a volume to each instance:
+
```
aws ec2 attach-volume --volume-id <volume-id> --instance-id <instance-id> --device /dev/sdf
```
+
For example:
+
```
aws ec2 attach-volume --volume-id vol-df734157 --instance-id i-b27ee627 --device /dev/sdk
aws ec2 attach-volume --volume-id vol-ce734146 --instance-id i-b37ee626 --device /dev/sdk
aws ec2 attach-volume --volume-id vol-ec734164 --instance-id i-b47ee621 --device /dev/sdk
aws ec2 attach-volume --volume-id vol-cf734147 --instance-id i-b57ee620 --device /dev/sdk
```
+
. For each minion (because Kubernetes 1.4.5 uses 1.11.2, works OOTB on 1.12)
.. ssh minion: `ssh -i ~/.ssh/kube_aws_rsa ubuntu@<public-ip>`
.. Use this command for default Debian: `ssh -i ~/.ssh/kube_aws_rsa admin@<public-ip>`
.. `sudo vi /lib/systemd/system/docker.service`
.. Comment `MountFlags=slave`
.. `sudo systemctl daemon-reload`
.. `sudo systemctl restart docker`
. On each host, start PWX container:
+
```
sudo docker run --restart=always --name px -d --net=host \
   --privileged=true \
   -v /run/docker/plugins:/run/docker/plugins \
   -v /var/lib/osd:/var/lib/osd:shared \
   -v /dev:/dev \
   -v /etc/pwx:/etc/pwx \
   -v /opt/pwx/bin:/export_bin \
   -v /usr/libexec/kubernetes/kubelet-plugins/volume/exec/px~flexvolume:/export_flexvolume:shared \
   -v /var/run/docker.sock:/var/run/docker.sock \
   -v /var/cores:/var/cores \
   -v /var/lib/kubelet:/var/lib/kubelet:shared \
   --ipc=host \
   portworx/px-enterprise -k etcd:http://etcd-us-east-1b.portworx.com:4001 -c couchbase_portworx2 -a -f
```
+
.. Check the status:
+
```
sudo /opt/pwx/bin/pxctl status
Status: PX is operational
Node ID: 54b41d07-7508-4167-8438-32faf390263e
	IP: 172.20.0.231 
 	Local Storage Pool: 1 pool
	Pool	Cos		Size	Used	Status	Zone	Region
	0	COS_TYPE_LOW	128 GiB	2.0 GiB	Online	a	us-west-2
	Local Storage Devices: 1 device
	Device	Path		Media Type		Size		Last-Scan
	0:1	/dev/xvdk	STORAGE_MEDIUM_SSD	128 GiB		07 Dec 16 00:19 UTC
	total			-			128 GiB
Cluster Summary
	Cluster ID: couchbase_portworx2
	Node IP: 172.20.0.231 - Capacity: 0 B/128 GiB Online (This node)
Global Storage Pool
	Total Used    	:  0 B
	Total Capacity	:  128 GiB
```
+
After all four nodes are provisioned with PWX container, status looks like:
+
```
sudo /opt/pwx/bin/pxctl status
Status: PX is operational
Node ID: aa8b5ce9-dc23-4b98-b5c4-491b9edb619e
	IP: 172.20.0.229 
 	Local Storage Pool: 1 pool
	Pool	Cos		Size	Used	Status	Zone	Region
	0	COS_TYPE_LOW	128 GiB	2.0 GiB	Online	a	us-west-2
	Local Storage Devices: 1 device
	Device	Path		Media Type		Size		Last-Scan
	0:1	/dev/xvdk	STORAGE_MEDIUM_SSD	128 GiB		07 Dec 16 00:30 UTC
	total			-			128 GiB
Cluster Summary
	Cluster ID: couchbase_portworx2
	Node IP: 172.20.0.228 - Capacity: 2.0 GiB/128 GiB Online
	Node IP: 172.20.0.231 - Capacity: 2.0 GiB/128 GiB Online
	Node IP: 172.20.0.230 - Capacity: 2.0 GiB/128 GiB Online
	Node IP: 172.20.0.229 - Capacity: 2.0 GiB/128 GiB Online (This node)
Global Storage Pool
	Total Used    	:  8.1 GiB
	Total Capacity	:  512 GiB
```
+
. Create 2 PWX volumes (on any Kubernetes worker host) - volumes are visible cluster-wide
.. `sudo /opt/pwx/bin/pxctl volume create couchbase1`
.. `sudo /opt/pwx/bin/pxctl volume create couchbase2`
. Create 2 PV - make sure to change `name` and `volumeID` attribute values
.. `./cluster/kubectl.sh create -f /Users/arungupta/workspaces/couchbase-kubernetes/cluster-petset-portworx/pv.yaml`
.. `./cluster/kubectl.sh create -f /Users/arungupta/workspaces/couchbase-kubernetes/cluster-petset-portworx/pv.yaml`
. 

== Couchbase with Portworx on Kubernetes


== Misc

. Optional verification
.. Log in to minion: `ssh -i ~/.ssh/kube_aws_rsa admin@<master-ip-public>`
.. Verify etcd: `curl -fs -X PUT "http://<master-ip-internal>:2379/v2/keys/_test"`
